#!/usr/bin/env python3
"""advanced_rag.py

Enterprise-grade Retrieval-Augmented Generation (RAG)
=====================================================
This script shows a *reference* implementation of an on-prem / private-cloud
RAG system that tackles several real-world enterprise requirements:

1. **Document ingestion & vectorisation** with LangChain + HuggingFace
2. **ChromaDB** for fast similarity search with persistence on disk
3. **Fine-grained access control (ACL)** at query-time via metadata filters
4. **Hallucination / Confidence scoring** to flag low-support answers
5. **Dynamic content update** â€“ re-index only changed files
6. **Scalability hooks** â€“ multiprocessing batch embeds, env-driven configs

The code intentionally keeps external dependencies minimal while staying
framework-agnostic. Adapt to your own auth system, LLM provider, queue, etc.
"""
from __future__ import annotations

import argparse
import glob
import json
import os
import sys
import time

from pathlib import Path
from typing import List, Sequence

from langchain.document_loaders import TextLoader
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.schema import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.vectorstores import Chroma
from langchain.llms import HuggingFaceHub
from langchain.chains import RetrievalQA
from langchain.schema.retriever import BaseRetriever

# ---------------------------------------------------------------------------
# Configuration helpers
# ---------------------------------------------------------------------------

def env(key: str, default: str | None = None) -> str | None:
    """Tiny helper to read env vars with default."""
    return os.environ.get(key, default)


# ---------------------------------------------------------------------------
# Data ingestion utilities
# ---------------------------------------------------------------------------

DEFAULT_SEPARATORS = ["\n\n", "\n", " "]


def discover_txt_files(folder: Path) -> List[Path]:
    pattern = str(folder / "**" / "*.txt")
    return [Path(p) for p in glob.glob(pattern, recursive=True)]


def read_acl_sidecar(txt_path: Path) -> list[str]:
    """Look for a JSON sidecar with ACL info next to the .txt file.

    E.g.  `policy.txt`  â†’  `policy.txt.meta`
    The sidecar *should* contain JSON like: {"acl": ["hr", "all"]}
    Missing file â‡’ open to all.
    """
    sidecar = txt_path.with_suffix(txt_path.suffix + ".meta")
    if not sidecar.exists():
        return ["*"]  # wildcard
    try:
        meta = json.loads(sidecar.read_text("utf-8"))
        acl = meta.get("acl", ["*"])
        if isinstance(acl, str):
            return [acl]
        return list(acl)
    except Exception as exc:  # noqa: BLE001
        print(f"[WARN] Could not parse ACL sidecar for {txt_path}: {exc}")
        return ["*"]


def load_and_split_docs(
    folder: Path,
    *,
    chunk_size: int = 1000,
    chunk_overlap: int = 100,
) -> list[Document]:
    txt_files = discover_txt_files(folder)
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=chunk_size,
        chunk_overlap=chunk_overlap,
        separators=DEFAULT_SEPARATORS,
    )
    docs: list[Document] = []
    for fp in txt_files:
        loader = TextLoader(str(fp), encoding="utf-8")
        file_docs = loader.load()  # returns List[Document]
        # attach metadata BEFORE splitting so children inherit it
        acl_roles = read_acl_sidecar(fp)
        for d in file_docs:
            d.metadata.update(
                {
                    "source": str(fp),
                    "acl": acl_roles,
                    "mtime": fp.stat().st_mtime,
                }
            )
        split_docs = splitter.split_documents(file_docs)
        docs.extend(split_docs)
    return docs


# ---------------------------------------------------------------------------
# Vector store wrapper with ACL-aware retrieval
# ---------------------------------------------------------------------------

class AclRetriever(BaseRetriever):
    """ê¸°ì¡´ ë²¡í„° ì €ì¥ì†Œë¥¼ ë˜í•‘í•˜ì—¬ ACL(ì ‘ê·¼ì œì–´) í•„í„°ë§ì„ ì ìš©í•˜ëŠ” í´ë˜ìŠ¤"""
    
    vectordb: Chroma                           # ChromaDB ë²¡í„° ì €ì¥ì†Œ
    allowed_roles: Sequence[str] = ["*"]       # í—ˆìš©ëœ ì‚¬ìš©ì ì—­í•  ëª©ë¡ (ê¸°ë³¸ê°’: ëª¨ë“  ì—­í• )
    top_k: int = 4                             # ë°˜í™˜í•  ìµœëŒ€ ë¬¸ì„œ ìˆ˜

    class Config:
        arbitrary_types_allowed = True             # Pydanticì—ì„œ ì„ì˜ íƒ€ì… í—ˆìš©

    def _is_authorised(self, doc: Document) -> bool:
        """ë¬¸ì„œì˜ ACL ë©”íƒ€ë°ì´í„°ë¥¼ í™•ì¸í•˜ì—¬ ì‚¬ìš©ì ì ‘ê·¼ ê¶Œí•œì„ ê²€ì¦"""
        doc_roles = doc.metadata.get("acl", ["*"])    # ë¬¸ì„œì˜ ì ‘ê·¼ ê¶Œí•œ ì—­í•  ëª©ë¡
        if "*" in doc_roles:                         # ì™€ì¼ë“œì¹´ë“œ('*')ëŠ” ëª¨ë“  ì‚¬ìš©ì í—ˆìš©
            return True
        # ì‚¬ìš©ì ì—­í• ê³¼ ë¬¸ì„œ ì—­í• ì˜ êµì§‘í•©ì´ ìˆìœ¼ë©´ ì ‘ê·¼ í—ˆìš©
        return bool(set(doc_roles) & set(self.allowed_roles))

    def similarity_search(self, query: str) -> list[Document]:
        """ìœ ì‚¬ë„ ê²€ìƒ‰ í›„ ACL í•„í„°ë§ì„ ì ìš©í•˜ì—¬ ê¶Œí•œì´ ìˆëŠ” ë¬¸ì„œë§Œ ë°˜í™˜"""
        # ì¶©ë¶„í•œ í›„ë³´ ë¬¸ì„œë¥¼ ê°€ì ¸ì˜´ (top_kì˜ 2ë°°)
        candidates = self.vectordb.similarity_search(query, k=self.top_k * 2)
        # ì ‘ê·¼ ê¶Œí•œì´ ìˆëŠ” ë¬¸ì„œë§Œ í•„í„°ë§
        authorised = [d for d in candidates if self._is_authorised(d)]
        return authorised[: self.top_k]               # ìµœì¢…ì ìœ¼ë¡œ top_kê°œë§Œ ë°˜í™˜

    # LangChain retriever ì¸í„°í˜ì´ìŠ¤ êµ¬í˜„
    def _get_relevant_documents(self, query: str) -> list[Document]:
        """LangChain í‘œì¤€ ì¸í„°í˜ì´ìŠ¤ë¥¼ ìœ„í•œ ë˜í¼ ë©”ì„œë“œ"""
        return self.similarity_search(query)


# ---------------------------------------------------------------------------
# Hallucination / confidence scoring
# ---------------------------------------------------------------------------

def compute_similarity(a_emb: list[float], b_emb: list[float]) -> float:
    """ë‘ ì„ë² ë”© ë²¡í„° ê°„ì˜ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ë¥¼ ê³„ì‚° (ì‘ì€ ë¦¬ìŠ¤íŠ¸ìš© ìµœì í™”)"""
    import numpy as np  # ì˜ì¡´ì„±ì„ ìµœì†Œí™”í•˜ê¸° ìœ„í•œ ì§€ì—­ import

    a = np.array(a_emb)                         # ì²« ë²ˆì§¸ ì„ë² ë”© ë²¡í„°
    b = np.array(b_emb)                         # ë‘ ë²ˆì§¸ ì„ë² ë”© ë²¡í„°
    denom = (np.linalg.norm(a) * np.linalg.norm(b)) or 1.0  # ë¶„ëª¨ (0 ë°©ì§€)
    dot_product = np.dot(a, b)                  # ë‚´ì  ê³„ì‚°
    return float(dot_product / denom)           # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ë°˜í™˜ (-1 ~ 1)


def hallucination_score(answer: str, context: list[Document], embedder) -> float:
    """í™˜ê°(Hallucination) íƒì§€ë¥¼ ìœ„í•œ ì ìˆ˜ ê³„ì‚°
    
    LLM ë‹µë³€ê³¼ ê²€ìƒ‰ëœ ë¬¸ì„œë“¤ì˜ ì„ë² ë”© ìœ ì‚¬ë„ë¥¼ ë¹„êµí•˜ì—¬ í™˜ê° ê°€ëŠ¥ì„±ì„ ì¸¡ì •
    
    ë°˜í™˜ê°’: [0, 2] ë²”ìœ„ì˜ ê±°ë¦¬ ì ìˆ˜
    - < 0.7: ê°•í•œ ê·¼ê±° ê¸°ë°˜ (ì‹ ë¢°ë„ ë†’ìŒ)
    - > 1.0: í™˜ê° ê°€ëŠ¥ì„± ìˆìŒ (ì‹ ë¢°ë„ ë‚®ìŒ)
    """
    if not context or not answer:
        return 1.0  # ì»¨í…ìŠ¤íŠ¸ë‚˜ ë‹µë³€ì´ ì—†ìœ¼ë©´ ì¤‘ê°„ê°’ ë°˜í™˜
        
    try:
        # LLM ë‹µë³€ì„ ì„ë² ë”©ìœ¼ë¡œ ë³€í™˜
        ans_emb = embedder.embed_query(answer)
        # ê²€ìƒ‰ëœ ëª¨ë“  ë¬¸ì„œì˜ ë‚´ìš©ì„ ì„ë² ë”©ìœ¼ë¡œ ë³€í™˜
        ctx_embs = [embedder.embed_query(doc.page_content) for doc in context]
        import numpy as np

        if not ctx_embs:
            return 1.0
            
        # ì»¨í…ìŠ¤íŠ¸ ì„ë² ë”©ë“¤ì˜ í‰ê·  ê³„ì‚° (ì „ì²´ ê²€ìƒ‰ ë§¥ë½ ëŒ€í‘œ)
        ctx_mean = np.mean(ctx_embs, axis=0)
        # ë‹µë³€ê³¼ ì»¨í…ìŠ¤íŠ¸ í‰ê·  ê°„ì˜ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
        sim = compute_similarity(ans_emb, ctx_mean.tolist())  # -1..1 ë²”ìœ„
        # ìœ ì‚¬ë„ë¥¼ ê±°ë¦¬ ì ìˆ˜ë¡œ ë³€í™˜ (0=ìµœê³ , 2=ìµœì•…)
        return 1.0 - ((sim + 1) / 2)
    except Exception:
        return 0.5  # ì˜¤ë¥˜ ë°œìƒ ì‹œ ì•ˆì „í•œ ê¸°ë³¸ê°’ ë°˜í™˜


# ---------------------------------------------------------------------------
# RAG engine
# ---------------------------------------------------------------------------

class RagEngine:
    """ê¸°ì—…ìš© RAG(ê²€ìƒ‰ ì¦ê°• ìƒì„±) ì—”ì§„ì˜ ë©”ì¸ í´ë˜ìŠ¤"""
    
    def __init__(self, collection_name: str = "rag_collection", persist_dir: str = "./chroma_db", 
                 embedding_model: str = "sentence-transformers/all-mpnet-base-v2"):
        """RAG ì—”ì§„ ì´ˆê¸°í™”
        
        Args:
            collection_name: ChromaDB ì»¬ë ‰ì…˜ ì´ë¦„
            persist_dir: ë²¡í„° DB ì €ì¥ ë””ë ‰í† ë¦¬
            embedding_model: HuggingFace ì„ë² ë”© ëª¨ë¸ëª…
        """
        self.collection_name = collection_name     # ë²¡í„° ì €ì¥ì†Œ ì»¬ë ‰ì…˜ëª…
        self.persist_dir = persist_dir             # ë””ìŠ¤í¬ ì €ì¥ ê²½ë¡œ
        self.embedding_model = embedding_model     # ì‚¬ìš©í•  ì„ë² ë”© ëª¨ë¸

        # í•µì‹¬ ì»´í¬ë„ŒíŠ¸ë“¤ ì´ˆê¸°í™”
        # HuggingFace ì„ë² ë”© ëª¨ë¸ ë¡œë“œ (ë¬¸ì„œì™€ ì§ˆì˜ë¥¼ ë²¡í„°ë¡œ ë³€í™˜)
        self.embedder = HuggingFaceEmbeddings(model_name=self.embedding_model)
        # ChromaDB ë²¡í„° ì €ì¥ì†Œ ì´ˆê¸°í™” (ìœ ì‚¬ë„ ê²€ìƒ‰ ë° ì˜êµ¬ ì €ì¥)
        self.vectordb = Chroma(
            collection_name=self.collection_name,     # ì»¬ë ‰ì…˜ êµ¬ë¶„ì
            embedding_function=self.embedder,         # ì„ë² ë”© í•¨ìˆ˜ ì—°ê²°
            persist_directory=self.persist_dir,       # ë””ìŠ¤í¬ ì €ì¥ ìœ„ì¹˜
        )

    # ------------------ ingestion ------------------
    def index_folder(
        self,
        folder: Path,
        *,
        chunk_size: int = 1000,
        chunk_overlap: int = 100,
        incremental: bool = True,
    ):
        """Index documents in *folder*.

        If *incremental* is True, only add files that were modified
        after their last index time (mtime stored in metadata).
        """
        all_docs = load_and_split_docs(folder, chunk_size=chunk_size, chunk_overlap=chunk_overlap)
        if not incremental or not list(self.vectordb.get()):
            self.vectordb.add_documents(all_docs)
        else:
            new_docs: list[Document] = []
            # Use Chroma metadata filter to see if we already indexed this mtime
            from datetime import datetime

            for doc in all_docs:
                src = doc.metadata.get("source")
                mtime = doc.metadata.get("mtime")
                records = self.vectordb.get(
                    where={"source": src, "mtime": mtime},
                    include=["metadatas"],
                )
                if len(records["ids"]) == 0:
                    new_docs.append(doc)
            if new_docs:
                print(f"Adding {len(new_docs)} new / updated chunks to index â€¦")
                self.vectordb.add_documents(new_docs)
        self.vectordb.persist()

    # ------------------ ì§ˆì˜ì‘ë‹µ ì²˜ë¦¬ ------------------
    def answer(
        self,
        query: str,
        *,
        user_roles: Sequence[str] = ("*",),           # ì‚¬ìš©ì ì—­í•  (ê¸°ë³¸: ëª¨ë“  ì—­í•  í—ˆìš©)
        top_k: int = 4,                               # ê²€ìƒ‰í•  ë¬¸ì„œ ìˆ˜
        llm_repo: str = "google/flan-t5-base",        # HuggingFace ëª¨ë¸ëª… (í´ë°±ìš©)
        hallucination_threshold: float = 0.35,       # í™˜ê° íƒì§€ ì„ê³„ê°’
    ) -> dict:
        """
        ì‚¬ìš©ì ì§ˆì˜ì— ëŒ€í•œ RAG ê¸°ë°˜ ë‹µë³€ ìƒì„±
        
        Args:
            query: ì‚¬ìš©ì ì§ˆë¬¸
            user_roles: ì ‘ê·¼ ê¶Œí•œì´ ìˆëŠ” ì—­í•  ëª©ë¡
            top_k: ìœ ì‚¬ë„ ê²€ìƒ‰í•  ë¬¸ì„œ ê°œìˆ˜
            llm_repo: HuggingFace ëª¨ë¸ ì €ì¥ì†Œëª… (API í‚¤ ì—†ì„ ì‹œ ì‚¬ìš©)
            hallucination_threshold: í™˜ê° íŒì • ì„ê³„ê°’ (0.35 = 35%)
            
        Returns:
            dict: {
                'answer': ìƒì„±ëœ ë‹µë³€,
                'sources': ì°¸ì¡° ë¬¸ì„œ ë©”íƒ€ë°ì´í„° ëª©ë¡,
                'confidence': ì‹ ë¢°ë„ ì ìˆ˜ (0~1),
                'hallucination_flag': í™˜ê° ê²½ê³  í”Œë˜ê·¸
            }
        """
        # RBAC í•„í„°ë§ì´ ì ìš©ëœ ê²€ìƒ‰ê¸° ìƒì„±
        retriever = AclRetriever(vectordb=self.vectordb, allowed_roles=user_roles, top_k=top_k)
        
        # LLM ì„ íƒ ìš°ì„ ìˆœìœ„: Gemini > OpenAI > HuggingFace > Fake
        import os
        if os.getenv("GOOGLE_API_KEY"):
            # Google Gemini 2.0 Flash ì‚¬ìš© (1ìˆœìœ„ - ìµœì‹  ì‹¤í—˜ ëª¨ë¸)
            from langchain_google_genai import ChatGoogleGenerativeAI
            llm = ChatGoogleGenerativeAI(
                model="gemini-2.0-flash-exp",            # ìµœì‹  ì‹¤í—˜ ë²„ì „
                google_api_key=os.getenv("GOOGLE_API_KEY"),
                temperature=0.0,                         # ê²°ì •ì  ì‘ë‹µ (ë³€ë™ì„± ìµœì†Œí™”)
                max_tokens=512                           # ì‘ë‹µ ê¸¸ì´ ì œí•œ
            )
        elif os.getenv("OPENAI_API_KEY"):
            # OpenAI GPT ì‚¬ìš© (2ìˆœìœ„)
            from langchain.llms import OpenAI
            llm = OpenAI(temperature=0.0, max_tokens=512)
        elif os.getenv("HUGGINGFACEHUB_API_TOKEN"):
            # HuggingFace Hub ì˜¤í”ˆì†ŒìŠ¤ ëª¨ë¸ ì‚¬ìš© (3ìˆœìœ„)
            from langchain.llms import HuggingFaceHub
            llm = HuggingFaceHub(
                repo_id=llm_repo,                        # ì§€ì •ëœ ëª¨ë¸ ì €ì¥ì†Œ
                model_kwargs={"temperature": 0.1, "max_length": 512},
                task="text2text-generation"              # Text-to-Text ìƒì„± íƒœìŠ¤í¬
            )
        else:
            # í´ë°±: API í‚¤ ì—†ì„ ì‹œ í…ŒìŠ¤íŠ¸ìš© ê°€ì§œ ì‘ë‹µ ìƒì„±ê¸°
            from langchain.llms.fake import FakeListLLM
            llm = FakeListLLM(responses=[
                "ê²€ìƒ‰ëœ ë¬¸ì„œì— ë”°ë¥´ë©´, ê³ ê°ë“¤ì´ ì£¼ë¡œ ë¶ˆë§Œì„ ì œê¸°í•œ ê¸°ëŠ¥ì€ ëŠë¦° ì‘ë‹µ ì‹œê°„, ë³µì¡í•œ ì‚¬ìš©ì ì¸í„°í˜ì´ìŠ¤, í†µí•© ê¸°ëŠ¥ ë¶€ì¡±ì…ë‹ˆë‹¤.",
                "ì£¼ìš” ê³ ê° ë¶ˆë§Œì‚¬í•­ì€ ì„±ëŠ¥ ë¬¸ì œ, ì‚¬ìš©ì„± ë¬¸ì œ, ëˆ„ë½ëœ ê¸°ëŠ¥ì— ì§‘ì¤‘ë˜ì–´ ìˆìŠµë‹ˆë‹¤.", 
                "ë¬¸ì„œì— ë”°ë¥´ë©´ ë¹ˆë²ˆí•œ ë¬¸ì œë¡œëŠ” ì‹œìŠ¤í…œ ì§€ì—°, ë¶€ì‹¤í•œ íƒìƒ‰ ê¸°ëŠ¥, ë¶ˆì¶©ë¶„í•œ API ì§€ì› ë“±ì´ ìˆìŠµë‹ˆë‹¤."
            ])
            
        # LangChain RetrievalQA ì²´ì¸ êµ¬ì„±
        qa_chain = RetrievalQA.from_chain_type(
            llm=llm,                                     # ì„ íƒëœ LLM ëª¨ë¸
            retriever=retriever,                         # RBAC ì ìš©ëœ ê²€ìƒ‰ê¸°
            chain_type="stuff",                          # ëª¨ë“  ê²€ìƒ‰ ë¬¸ì„œë¥¼ í•˜ë‚˜ë¡œ í•©ì³ì„œ í”„ë¡¬í”„íŠ¸ì— ì‚½ì…
        )
        
        # ì‹¤ì œ ë‹µë³€ ìƒì„± ì‹¤í–‰
        answer = qa_chain.run(query)
        
        # ê²€ìƒ‰ëœ ë¬¸ì„œë“¤ (í™˜ê° íƒì§€ìš©)
        docs = retriever.similarity_search(query)
        
        # í™˜ê° ì ìˆ˜ ê³„ì‚° (ë‹µë³€ê³¼ ê²€ìƒ‰ ë¬¸ì„œ ê°„ ìœ ì‚¬ë„ ê¸°ë°˜)
        score = hallucination_score(answer, docs, self.embedder)
        
        # ìµœì¢… ì‘ë‹µ ë”•ì…”ë„ˆë¦¬ êµ¬ì„±
        return {
            "answer": answer,                           # ìƒì„±ëœ ë‹µë³€
            "sources": [d.metadata for d in docs],     # ì°¸ì¡° ë¬¸ì„œ ë©”íƒ€ë°ì´í„° ëª©ë¡
            "confidence": 1 - score,                   # ì‹ ë¢°ë„ (1 - í™˜ê°ì ìˆ˜)
            "hallucination_flag": score > hallucination_threshold,  # í™˜ê° ê²½ê³  í”Œë˜ê·¸
        }


# ---------------------------------------------------------------------------
# CLI
# ---------------------------------------------------------------------------

def build_cli():
    parser = argparse.ArgumentParser(description="Enterprise RAG reference implementation")
    sub = parser.add_subparsers(dest="command", required=True)

    # index
    p_index = sub.add_parser("index", help="Index a folder of .txt docs")
    p_index.add_argument("folder", type=Path)
    p_index.add_argument("--chunk_size", type=int, default=1000)
    p_index.add_argument("--chunk_overlap", type=int, default=100)
    p_index.add_argument("--no_incremental", action="store_true")

    # query
    p_query = sub.add_parser("query", help="Ask a question via RAG")
    p_query.add_argument("question")
    p_query.add_argument("--roles", default="*", help="Comma-separated user roles (ACL)")
    p_query.add_argument("--top_k", type=int, default=4)
    p_query.add_argument("--llm_repo", default="google/flan-t5-base")
    p_query.add_argument("--verbose", "-v", action="store_true", help="Show detailed JSON output")

    return parser


def main(argv: list[str] | None = None):
    parser = build_cli()
    args = parser.parse_args(argv)

    engine = RagEngine(
        collection_name=env("RAG_COLLECTION", "herb_collection"),
        persist_dir=env("RAG_PERSIST_DIR", "./chroma_db"),
        embedding_model=env("RAG_EMBED_MODEL", "sentence-transformers/all-mpnet-base-v2"),
    )

    if args.command == "index":
        engine.index_folder(
            args.folder,
            chunk_size=args.chunk_size,
            chunk_overlap=args.chunk_overlap,
            incremental=not args.no_incremental,
        )
        print("Indexing completed.")

    elif args.command == "query":
        roles = [r.strip() for r in args.roles.split(",") if r.strip()]
        start = time.perf_counter()
        res = engine.answer(
            args.question,
            user_roles=roles,
            top_k=args.top_k,
            llm_repo=args.llm_repo,
        )
        dur = time.perf_counter() - start
        
        # ê¹”ë”í•œ ì¶œë ¥ í˜•ì‹
        print(f"ğŸ“‹ ì§ˆë¬¸: {args.question}")
        print()
        print(f"ğŸ’¡ ë‹µë³€: {res['answer']}")
        print()
        print(f"ğŸ“Š ì‹ ë¢°ë„: {res['confidence']:.1%}")
        print(f"ğŸ” ê´€ë ¨ ë¬¸ì„œ: {len(res['sources'])}ê°œ")
        print(f"â±ï¸ ì²˜ë¦¬ ì‹œê°„: {dur:.1f}ì´ˆ")
        
        if res['hallucination_flag']:
            print("âš ï¸  ì£¼ì˜: í™˜ê° ê°€ëŠ¥ì„± ìˆìŒ")
        
        # ìƒì„¸ ì •ë³´ê°€ í•„ìš”í•œ ê²½ìš° JSON ì¶œë ¥
        if args.verbose:
            print("\n" + "="*50)
            print("ìƒì„¸ ì •ë³´:")
            print(json.dumps({**res, "elapsed_sec": dur}, indent=2, ensure_ascii=False))


if __name__ == "__main__":
    main()
