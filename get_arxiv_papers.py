#!/usr/bin/env python3
# get_arxiv_papers.py
import requests
import xml.etree.ElementTree as ET
import json
import gzip
import time
import re
from datetime import datetime, date
import os

# arXiv API ì„¤ì •
ARXIV_API = "http://export.arxiv.org/api/query"
BATCH_SIZE = 100
TARGET_SIZE_MB = 400
OUT_PATH = "./arxiv_papers_2024_25.jsonl.gz"

# ê´€ì‹¬ ì¹´í…Œê³ ë¦¬ (AI/ML ê´€ë ¨)
CATEGORIES = [
    "cs.AI", "cs.LG", "cs.CL", "cs.CV", "cs.NE", 
    "stat.ML", "cs.IR", "cs.RO", "cs.HC"
]

def parse_arxiv_entry(entry):
    """arXiv XML entryë¥¼ íŒŒì‹±í•´ì„œ dictë¡œ ë³€í™˜"""
    ns = {'atom': 'http://www.w3.org/2005/Atom'}
    
    # ê¸°ë³¸ ì •ë³´ ì¶”ì¶œ
    paper = {}
    paper["arxiv_id"] = entry.find('atom:id', ns).text.split('/')[-1]
    paper["title"] = entry.find('atom:title', ns).text.strip()
    paper["abstract"] = entry.find('atom:summary', ns).text.strip()
    
    # ë‚ ì§œ ì¶”ì¶œ
    published = entry.find('atom:published', ns).text
    paper["published_date"] = published[:10]  # YYYY-MM-DD
    
    # ì €ì ì¶”ì¶œ
    authors = []
    for author in entry.findall('atom:author', ns):
        name = author.find('atom:name', ns)
        if name is not None:
            authors.append(name.text)
    paper["authors"] = authors
    
    # ì¹´í…Œê³ ë¦¬ ì¶”ì¶œ
    categories = []
    for cat in entry.findall('atom:category', ns):
        term = cat.get('term')
        if term:
            categories.append(term)
    paper["categories"] = categories
    
    return paper

def get_papers_by_category(category, start_date="2024-01-01", max_results=1000):
    """íŠ¹ì • ì¹´í…Œê³ ë¦¬ì—ì„œ ë…¼ë¬¸ ìˆ˜ì§‘"""
    papers = []
    start = 0
    
    while len(papers) < max_results:
        params = {
            'search_query': f'cat:{category} AND submittedDate:[{start_date} TO 2025-12-31]',
            'start': start,
            'max_results': min(BATCH_SIZE, max_results - len(papers)),
            'sortBy': 'submittedDate',
            'sortOrder': 'descending'
        }
        
        print(f"  ğŸ“„ {category}: ìš”ì²­ ì¤‘... (start={start})")
        
        try:
            response = requests.get(ARXIV_API, params=params, timeout=30)
            response.raise_for_status()
            
            root = ET.fromstring(response.content)
            entries = root.findall('{http://www.w3.org/2005/Atom}entry')
            
            if not entries:
                print(f"  âœ… {category}: ë” ì´ìƒ ê²°ê³¼ ì—†ìŒ")
                break
                
            batch_papers = []
            for entry in entries:
                paper = parse_arxiv_entry(entry)
                # 2024-2025ë…„ë§Œ í•„í„°ë§
                if paper["published_date"].startswith(("2024", "2025")):
                    batch_papers.append(paper)
            
            papers.extend(batch_papers)
            print(f"  ğŸ“Š {category}: {len(batch_papers)}í¸ ì¶”ê°€ (ì´ {len(papers)}í¸)")
            
            start += BATCH_SIZE
            time.sleep(1)  # API ì œí•œ ì¤€ìˆ˜
            
        except Exception as e:
            print(f"  âŒ {category} ì˜¤ë¥˜: {e}")
            break
    
    return papers

def main():
    print("ğŸ“š arXivì—ì„œ 2024-2025 AI/ML ë…¼ë¬¸ ìˆ˜ì§‘ ì¤‘...")
    print(f"ğŸ¯ ëª©í‘œ: {TARGET_SIZE_MB} MB")
    print(f"ğŸ“ ì €ì¥: {OUT_PATH}")
    
    all_papers = []
    
    for category in CATEGORIES:
        print(f"\nğŸ” ì¹´í…Œê³ ë¦¬: {category}")
        papers = get_papers_by_category(category, max_results=500)
        all_papers.extend(papers)
        
        # ì¤‘ê°„ ì²´í¬
        if len(all_papers) > 5000:  # ë„ˆë¬´ ë§ìœ¼ë©´ ì¤‘ë‹¨
            print(f"âš ï¸  ì¶©ë¶„í•œ ë°ì´í„° ìˆ˜ì§‘ë¨ ({len(all_papers)}í¸)")
            break
    
    # ì¤‘ë³µ ì œê±° (arxiv_id ê¸°ì¤€)
    unique_papers = {}
    for paper in all_papers:
        unique_papers[paper["arxiv_id"]] = paper
    
    final_papers = list(unique_papers.values())
    print(f"\nğŸ§¹ ì¤‘ë³µ ì œê±° í›„: {len(final_papers)}í¸")
    
    # ë‚ ì§œìˆœ ì •ë ¬ (ìµœì‹ ìˆœ)
    final_papers.sort(key=lambda x: x["published_date"], reverse=True)
    
    # gzipìœ¼ë¡œ ì €ì¥
    print(f"ğŸ’¾ {OUT_PATH}ì— ì €ì¥ ì¤‘...")
    with gzip.open(OUT_PATH, "wt", encoding="utf-8") as f_out:
        for i, paper in enumerate(final_papers):
            f_out.write(json.dumps(paper, ensure_ascii=False) + "\n")
            
            if (i + 1) % 500 == 0:
                size_mb = os.path.getsize(OUT_PATH) / (1024**2)
                print(f"  ğŸ“Š {i+1}í¸ ì €ì¥ ì™„ë£Œ, {size_mb:.1f} MB")
    
    size_mb = os.path.getsize(OUT_PATH) / (1024**2)
    print(f"\nğŸ‰ ìˆ˜ì§‘ ì™„ë£Œ!")
    print(f"ğŸ“Š ì´ {len(final_papers)}í¸ì˜ ë…¼ë¬¸ì„ {size_mb:.1f} MBë¡œ ì €ì¥")
    print(f"ğŸ’¡ í‰ê·  ë…¼ë¬¸ë‹¹ í¬ê¸°: {size_mb*1024/len(final_papers):.1f} KB")
    
    # ì—°ë„ë³„ ë¶„í¬ ì¶œë ¥
    year_count = {}
    for paper in final_papers:
        year = paper["published_date"][:4]
        year_count[year] = year_count.get(year, 0) + 1
    
    print(f"ğŸ“… ì—°ë„ë³„ ë¶„í¬: {year_count}")

if __name__ == "__main__":
    main() 