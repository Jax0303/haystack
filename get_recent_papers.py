#!/usr/bin/env python3
# get_recent_papers.py
import requests
import json
import gzip
import time
from datetime import datetime
import os

# Semantic Scholar API ì„¤ì •
API_BASE = "https://api.semanticscholar.org/graph/v1"
PAPERS_PER_BATCH = 100  # API ì œí•œì— ë§ì¶° ì¡°ì •
TOTAL_TARGET = 5000     # ëª©í‘œ ë…¼ë¬¸ ìˆ˜ (400MB ì •ë„)
OUT_PATH = "./recent_papers_2024_25.jsonl.gz"

# í•„ìš”í•œ í•„ë“œ
FIELDS = [
    "paperId", "title", "abstract", "year", "publicationDate", 
    "venue", "authors", "fieldsOfStudy", "citationCount"
]

def get_papers_batch(offset=0, limit=100):
    """ìµœì‹  ë…¼ë¬¸ì„ ë°°ì¹˜ë¡œ ê°€ì ¸ì˜¤ê¸°"""
    url = f"{API_BASE}/paper/search"
    params = {
        "query": "machine learning OR artificial intelligence OR deep learning",
        "year": "2024-2025",  # 2024-2025ë…„ë§Œ
        "fields": ",".join(FIELDS),
        "offset": offset,
        "limit": limit
    }
    
    try:
        response = requests.get(url, params=params, timeout=30)
        response.raise_for_status()
        return response.json()
    except Exception as e:
        print(f"âŒ API ì˜¤ë¥˜ (offset={offset}): {e}")
        return None

def main():
    print("ğŸ” Semantic Scholar APIë¡œ 2024-2025 ë…¼ë¬¸ ìˆ˜ì§‘ ì¤‘...")
    print(f"ğŸ“ ì €ì¥ ê²½ë¡œ: {OUT_PATH}")
    
    papers_collected = 0
    offset = 0
    
    with gzip.open(OUT_PATH, "wt", encoding="utf-8") as f_out:
        while papers_collected < TOTAL_TARGET:
            print(f"ğŸ“Š ë°°ì¹˜ ìš”ì²­ ì¤‘... (offset={offset}, ìˆ˜ì§‘ëœ ë…¼ë¬¸={papers_collected})")
            
            result = get_papers_batch(offset, PAPERS_PER_BATCH)
            if not result or "data" not in result:
                print("âŒ ë” ì´ìƒ ë°ì´í„°ê°€ ì—†ê±°ë‚˜ API ì˜¤ë¥˜")
                break
            
            papers = result["data"]
            if not papers:
                print("ğŸ“ ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤")
                break
                
            for paper in papers:
                # 2024-2025ë…„ ë…¼ë¬¸ë§Œ í™•ì‹¤íˆ í•„í„°ë§
                pub_date = paper.get("publicationDate", "")
                year = paper.get("year", 0)
                
                if year >= 2024 or (pub_date and pub_date.startswith(("2024", "2025"))):
                    # í•„ìš”í•œ ë°ì´í„°ë§Œ ì €ì¥
                    clean_paper = {
                        "paper_id": paper.get("paperId", ""),
                        "title": paper.get("title", ""),
                        "abstract": paper.get("abstract", ""),
                        "year": year,
                        "publication_date": pub_date,
                        "venue": paper.get("venue", ""),
                        "authors": [a.get("name", "") for a in paper.get("authors", [])],
                        "fields": paper.get("fieldsOfStudy", []),
                        "citations": paper.get("citationCount", 0)
                    }
                    
                    f_out.write(json.dumps(clean_paper, ensure_ascii=False) + "\n")
                    papers_collected += 1
                    
                    if papers_collected % 100 == 0:
                        size_mb = os.path.getsize(OUT_PATH) / (1024**2)
                        print(f"  âœ… {papers_collected}í¸ ìˆ˜ì§‘ ì™„ë£Œ, {size_mb:.1f} MB")
            
            offset += PAPERS_PER_BATCH
            
            # API ì œí•œ ì¤€ìˆ˜ (1ì´ˆ ëŒ€ê¸°)
            time.sleep(1)
            
            # ì´ ë…¼ë¬¸ ìˆ˜ê°€ ëª©í‘œì— ë„ë‹¬í•˜ë©´ ì¢…ë£Œ
            if papers_collected >= TOTAL_TARGET:
                break
    
    size_mb = os.path.getsize(OUT_PATH) / (1024**2)
    print(f"\nğŸ‰ ìˆ˜ì§‘ ì™„ë£Œ!")
    print(f"ğŸ“Š ì´ {papers_collected}í¸ì˜ ë…¼ë¬¸ì„ {size_mb:.1f} MBë¡œ ì €ì¥")
    print(f"ğŸ’¡ í‰ê·  ë…¼ë¬¸ë‹¹ í¬ê¸°: {size_mb*1024/papers_collected:.1f} KB")

if __name__ == "__main__":
    main() 