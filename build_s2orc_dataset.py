#!/usr/bin/env python3
"""
Pipeline: S2ORC â†’ PDF â†’ text  (â‰ˆ30-minute parsing dataset)

Steps
1. Get latest S2ORC release ID from Semantic Scholar API.
2. Download shard_0 â€“ shard_3 tar.gz (â‰ˆ2.6 GB) into ./s2orc_raw
3. Extract PDF files to ./s2orc_pdf
4. Parse each PDF with pdfminer.six â†’ text (cap 20 000 chars)
5. Keep docs mentioning 2024 or 2025 to maximise freshness.
6. Save into ./s2orc_2024_25_fulltext.jsonl.gz

Before running, export your API key:
  export S2_API_KEY="<YOUR_KEY>"

And install deps once:
  pip install requests tqdm pdfminer.six
"""
import os, re, json, tarfile, gzip, time
from pathlib import Path
from multiprocessing import Pool, cpu_count
from typing import List

import requests
from tqdm import tqdm
from pdfminer.high_level import extract_text

API_KEY = os.getenv("S2_API_KEY")
if not API_KEY:
    raise RuntimeError("âŒ í™˜ê²½ ë³€ìˆ˜ S2_API_KEY ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. Semantic Scholar API í‚¤ë¥¼ export í•´ì£¼ì„¸ìš”.")

RAW_DIR   = Path("s2orc_raw")
PDF_DIR   = Path("s2orc_pdf")
OUT_FILE  = Path("s2orc_2024_25_fulltext.jsonl.gz")
TARGET_SHARDS = [0, 1, 2, 3]  # shard indices to download (ì´ 4ê°œ)
MAX_TXT_LEN   = 20000          # ê° ë…¼ë¬¸ í…ìŠ¤íŠ¸ ìµœëŒ€ ê¸¸ì´

RAW_DIR.mkdir(exist_ok=True)
PDF_DIR.mkdir(exist_ok=True)

HEADERS = {"x-api-key": API_KEY}
BASE = "https://api.semanticscholar.org/datasets/v1"


def get_latest_release() -> str:
    url = f"{BASE}/release/latest"
    return requests.get(url, headers=HEADERS, timeout=30).json()["release_id"]


def list_s2orc_files(release_id: str) -> List[str]:
    url = f"{BASE}/release/{release_id}/dataset/s2orc/"
    data = requests.get(url, headers=HEADERS, timeout=30).json()
    return data["files"]


def download_file(url: str, dest: Path):
    with requests.get(url, stream=True, timeout=60) as r:
        r.raise_for_status()
        total = int(r.headers.get("Content-Length", 0))
        with open(dest, "wb") as f, tqdm(total=total, unit="B", unit_scale=True, desc=dest.name) as pbar:
            for chunk in r.iter_content(chunk_size=8192):
                if chunk:
                    f.write(chunk)
                    pbar.update(len(chunk))


def step1_download_shards():
    print("ğŸ“¥ 1) S2ORC shard ë‹¤ìš´ë¡œë“œ ì¤‘...")
    release = get_latest_release()
    print(f"   ìµœì‹  release: {release}")
    files = list_s2orc_files(release)
    pattern = re.compile(r"shard_(\d+)\.tar\.gz$")
    for url in files:
        m = pattern.search(url)
        if not m:
            continue
        idx = int(m.group(1))
        if idx in TARGET_SHARDS:
            dest = RAW_DIR / f"shard_{idx}.tar.gz"
            if dest.exists():
                print(f"   âœ… {dest.name} ì´ë¯¸ ì¡´ì¬")
                continue
            download_file(url, dest)
    print("   âœ” shard ë‹¤ìš´ë¡œë“œ ì™„ë£Œ\n")


def step2_extract_pdfs():
    print("ğŸ“‚ 2) PDF ì¶”ì¶œ ì¤‘...")
    tars = list(RAW_DIR.glob("shard_*.tar.gz"))
    for tar_path in tars:
        with tarfile.open(tar_path, "r:gz") as tar:
            members = [m for m in tar.getmembers() if m.name.endswith(".pdf")]
            for m in tqdm(members, desc=f"extract {tar_path.name}"):
                out_path = PDF_DIR / Path(m.name).name
                if out_path.exists():
                    continue
                try:
                    f = tar.extractfile(m)
                    if f:
                        with open(out_path, "wb") as out_f:
                            out_f.write(f.read())
                except Exception:
                    continue
    print("   âœ” PDF ì¶”ì¶œ ì™„ë£Œ\n")


def parse_one(pdf_path: Path):
    try:
        text = extract_text(pdf_path, maxpages=20)  # ì œí•œ: ì²« 20p ì •ë„ë§Œ
        if not text:
            return None
        if "2024" not in text and "2025" not in text:
            return None
        return {
            "file": pdf_path.name,
            "text": text[:MAX_TXT_LEN]
        }
    except Exception:
        return None


def step3_parse_pdfs():
    print("ğŸ“ 3) PDF â†’ í…ìŠ¤íŠ¸ ë³€í™˜ ì¤‘ (ë³‘ë ¬)...")
    pdf_files = list(PDF_DIR.glob("*.pdf"))
    print(f"   ëŒ€ìƒ PDF: {len(pdf_files):,} ê°œ")
    n_proc = max(1, cpu_count() - 1)
    with Pool(n_proc) as pool, gzip.open(OUT_FILE, "wt", encoding="utf-8") as fout:
        for result in tqdm(pool.imap_unordered(parse_one, pdf_files, chunksize=10), total=len(pdf_files)):
            if result:
                fout.write(json.dumps(result, ensure_ascii=False) + "\n")
    print("   âœ” í…ìŠ¤íŠ¸ ì¶”ì¶œ & ì €ì¥ ì™„ë£Œ\n")


def main():
    t0 = time.time()
    step1_download_shards()
    step2_extract_pdfs()
    step3_parse_pdfs()
    elapsed = (time.time() - t0) / 60
    size_mb = OUT_FILE.stat().st_size / (1024**2)
    print(f"ğŸ íŒŒì´í”„ë¼ì¸ ì™„ë£Œ! ê²½ê³¼ {elapsed:.1f} ë¶„, ê²°ê³¼ {size_mb:.1f} MB â†’ {OUT_FILE}")


if __name__ == "__main__":
    main() 