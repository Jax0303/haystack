#!/usr/bin/env python3
"""
index_rag_dataset.py
--------------------
RAG (Retrieval-Augmented Generation) íŒŒì´í”„ë¼ì¸ í•µì‹¬ ìŠ¤í¬ë¦½íŠ¸

ì£¼ìš” ê¸°ëŠ¥:
1. JSONL ë°ì´í„°ì…‹ ë¡œë“œ (ë…¼ë¬¸ í…ìŠ¤íŠ¸)
2. í…ìŠ¤íŠ¸ë¥¼ ~1000í† í° ì²­í¬ë¡œ ë¶„í•  (ì¤‘ì²© í¬í•¨)
3. Sentence-Transformersë¡œ ì„ë² ë”© ë²¡í„° ìƒì„±
4. FAISS ì¸ë±ìŠ¤ êµ¬ì¶• (ì½”ì‚¬ì¸ ìœ ì‚¬ë„/ë‚´ì  ê²€ìƒ‰)
5. Gemini-Proë¥¼ í†µí•œ RAG ì§ˆì˜ì‘ë‹µ CLI ì œê³µ

ì‚¬ìš©ë²•:
  # ì˜ì¡´ì„± ì„¤ì¹˜
  pip install sentence-transformers faiss-cpu google-generativeai nltk
  python -c "import nltk; nltk.download('punkt')"
  
  # ì¸ë±ìŠ¤ ìƒì„± (ìµœì´ˆ 5-10ë¶„ ì†Œìš”)
  export GOOGLE_API_KEY="<YOUR_GEMINI_API_KEY>"
  python index_rag_dataset.py build
  
  # ì§ˆì˜ì‘ë‹µ
  python index_rag_dataset.py ask "What are the key challenges in AI alignment in 2025?"
"""
import sys, gzip, json, os, time, itertools, pickle
from pathlib import Path
from typing import List

import numpy as np
import faiss
from tqdm import tqdm
# PyTorch/Transformers ì¶©ëŒ í•´ê²°ì„ ìœ„í•œ ì•ˆì „í•œ ì„í¬íŠ¸
try:
    # CPU ì „ìš© ëª¨ë“œë¡œ PyTorch ì„¤ì •
    os.environ['CUDA_VISIBLE_DEVICES'] = ''
    os.environ['PYTORCH_DISABLE_CUDA'] = '1'
    from sentence_transformers import SentenceTransformer
    TRANSFORMERS_AVAILABLE = True
except Exception as e:
    print(f"âš ï¸ SentenceTransformers import failed: {e}")
    print("ğŸ“¦ Installing compatible versions...")
    import subprocess
    subprocess.run([sys.executable, "-m", "pip", "install", "--upgrade", "--force-reinstall", "torch==2.1.0", "transformers==4.35.0", "sentence-transformers==2.2.2"], check=False)
    try:
        from sentence_transformers import SentenceTransformer
        TRANSFORMERS_AVAILABLE = True
    except:
        TRANSFORMERS_AVAILABLE = False
        print("âŒ Failed to load SentenceTransformers. Using fallback embedding.")

import nltk
from nltk.tokenize import sent_tokenize

try:
    import google.generativeai as genai
except ImportError:
    genai = None

# === íŒŒì¼ ê²½ë¡œ ë° ì„¤ì • ìƒìˆ˜ ===
DATA_FILE = Path("rag_dataset_cv.jsonl.gz")  # ì…ë ¥ ë°ì´í„°ì…‹ (gzip ì••ì¶• JSONL)
INDEX_FILE = Path("rag_faiss.index")              # FAISS ì¸ë±ìŠ¤ ì €ì¥ íŒŒì¼
META_FILE  = Path("rag_meta.pkl")                 # ì²­í¬ í…ìŠ¤íŠ¸ ë©”íƒ€ë°ì´í„° (pickle)

# === í…ìŠ¤íŠ¸ ì²­í‚¹ ì„¤ì • ===
CHUNK_SIZE_TOK = 1000  # ì²­í¬ë‹¹ ëŒ€ëµì ì¸ í† í° ìˆ˜ (4ì â‰ˆ 1í† í°ìœ¼ë¡œ ì¶”ì •)
OVERLAP_TOK    = 120   # ì²­í¬ ê°„ ì¤‘ì²© í† í° ìˆ˜ (ë¬¸ë§¥ ì—°ì†ì„± ë³´ì¥)

# === ì²­í‚¹ ë°©ë²• ì„¤ì • ===
CHUNKING_METHOD = "semantic"  # "simple", "semantic", "agentic" ì¤‘ ì„ íƒ

# === ì„ë² ë”© ëª¨ë¸ ì„¤ì • ===
EMB_MODEL_NAME = "BAAI/bge-small-en"  # BGE-Small-EN: ê²½ëŸ‰í™”ëœ ë‹¤êµ­ì–´ ì„ë² ë”© ëª¨ë¸ (384ì°¨ì›)


def estimate_token_count(text: str) -> int:
    """
    í…ìŠ¤íŠ¸ì˜ ëŒ€ëµì ì¸ í† í° ìˆ˜ ì¶”ì •
    
    ê°„ë‹¨í•œ íœ´ë¦¬ìŠ¤í‹±: 4ë¬¸ì â‰ˆ 1í† í° (ì˜ì–´ ê¸°ì¤€)
    ì‹¤ì œ í† í¬ë‚˜ì´ì €ë³´ë‹¤ ë¹ ë¥´ì§€ë§Œ ê·¼ì‚¬ì¹˜ì„
    
    Args:
        text: ì…ë ¥ í…ìŠ¤íŠ¸
    
    Returns:
        ì¶”ì • í† í° ìˆ˜
    """
    return max(1, len(text) // 4)  # ìµœì†Œ 1í† í° ë³´ì¥


def chunk_text(text: str) -> List[str]:
    """
    ê¸´ í…ìŠ¤íŠ¸ë¥¼ ì¤‘ì²©ë˜ëŠ” ì²­í¬ë¡œ ë¶„í• 
    
    NLTKì˜ sent_tokenizeë¡œ ë¬¸ì¥ ë‹¨ìœ„ ë¶„í•  í›„,
    ì§€ì •ëœ í† í° ìˆ˜ì— ë§ì¶° ì²­í¬ë¥¼ ìƒì„±í•˜ë©°
    ì²­í¬ ê°„ ì¤‘ì²©ì„ í†µí•´ ë¬¸ë§¥ ì—°ì†ì„± ë³´ì¥
    
    Args:
        text: ë¶„í• í•  ì›ë³¸ í…ìŠ¤íŠ¸
    
    Returns:
        ì²­í¬ í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸ (ê° ì²­í¬ëŠ” ìµœëŒ€ 4000ìë¡œ ì œí•œ)
    """
    # NLTKë¡œ ë¬¸ì¥ ë‹¨ìœ„ ë¶„í• 
    sentences = sent_tokenize(text)
    chunks = []
    current = []      # í˜„ì¬ ì²­í¬ì— í¬í•¨ë  ë¬¸ì¥ë“¤
    current_len = 0   # í˜„ì¬ ì²­í¬ì˜ í† í° ìˆ˜
    
    for sent in sentences:
        sent_len = estimate_token_count(sent)
        
        # í† í° ìˆ˜ ì´ˆê³¼ ì‹œ í˜„ì¬ ì²­í¬ ì™„ì„±í•˜ê³  ìƒˆ ì²­í¬ ì‹œì‘
        if current_len + sent_len > CHUNK_SIZE_TOK:
            if current:
                # í˜„ì¬ ì²­í¬ë¥¼ ìµœëŒ€ 4000ìë¡œ ì œí•œí•˜ì—¬ ì €ì¥
                chunks.append(" ".join(current)[:4000])
            
            # ì¤‘ì²©(overlap) ì²˜ë¦¬: ì´ì „ ì²­í¬ì˜ ë§ˆì§€ë§‰ ë¶€ë¶„ì„ ìƒˆ ì²­í¬ ì‹œì‘ìœ¼ë¡œ ë³µì‚¬
            overlap_tokens = OVERLAP_TOK // 4  # í† í°ì„ ë¬¸ì ìˆ˜ë¡œ ê·¼ì‚¬ ë³€í™˜
            overlap_text = current[-overlap_tokens:] if overlap_tokens < len(current) else current
            current = overlap_text.copy() if isinstance(overlap_text, list) else list(overlap_text)
            current_len = estimate_token_count(" ".join(current))
        
        # í˜„ì¬ ë¬¸ì¥ì„ ì²­í¬ì— ì¶”ê°€
        current.append(sent)
        current_len += sent_len
    
    # ë§ˆì§€ë§‰ ì²­í¬ ì²˜ë¦¬
    if current:
        chunks.append(" ".join(current)[:4000])
    
    return chunks


def semantic_chunk_text(text: str, model: SentenceTransformer, similarity_threshold: float = 0.5) -> List[str]:
    """
    ì˜ë¯¸ì  ìœ ì‚¬ë„ ê¸°ë°˜ í…ìŠ¤íŠ¸ ì²­í‚¹
    
    ë¬¸ì¥ ê°„ ì„ë² ë”© ìœ ì‚¬ë„ë¥¼ ê³„ì‚°í•˜ì—¬ ì˜ë¯¸ì ìœ¼ë¡œ ì—°ê´€ëœ ë¬¸ì¥ë“¤ì„ 
    ê°™ì€ ì²­í¬ë¡œ ê·¸ë£¹í™”. ìœ ì‚¬ë„ê°€ ì„ê³„ê°’ ì´í•˜ë¡œ ë–¨ì–´ì§€ë©´ ìƒˆ ì²­í¬ ì‹œì‘.
    
    Args:
        text: ë¶„í• í•  ì›ë³¸ í…ìŠ¤íŠ¸
        model: ì„ë² ë”© ìƒì„±ìš© SentenceTransformer ëª¨ë¸
        similarity_threshold: ì²­í¬ ë¶„ë¦¬ ì„ê³„ê°’ (0-1, ë‚®ì„ìˆ˜ë¡ ë” ì„¸ë¶„í™”)
    
    Returns:
        ì˜ë¯¸ì ìœ¼ë¡œ ì¼ê´€ì„± ìˆëŠ” ì²­í¬ í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
    """
    # ë¬¸ì¥ ë‹¨ìœ„ ë¶„í• 
    sentences = sent_tokenize(text)
    if len(sentences) <= 1:
        return [text[:4000]]
    
    # ëª¨ë“  ë¬¸ì¥ì˜ ì„ë² ë”© ìƒì„± (ë°°ì¹˜ ì²˜ë¦¬ë¡œ íš¨ìœ¨ì„± í–¥ìƒ)
    embeddings = model.encode(sentences, convert_to_numpy=True, normalize_embeddings=True)
    
    chunks = []
    current_chunk = [sentences[0]]  # ì²« ë²ˆì§¸ ë¬¸ì¥ìœ¼ë¡œ ì‹œì‘
    current_tokens = estimate_token_count(sentences[0])
    
    for i in range(1, len(sentences)):
        sent = sentences[i]
        sent_tokens = estimate_token_count(sent)
        
        # ì´ì „ ë¬¸ì¥ê³¼ì˜ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚° (ì •ê·œí™”ëœ ë²¡í„°ì˜ ë‚´ì )
        similarity = np.dot(embeddings[i-1], embeddings[i])
        
        # ìœ ì‚¬ë„ê°€ ì„ê³„ê°’ ì´ìƒì´ê³  í† í° ìˆ˜ê°€ í—ˆìš© ë²”ìœ„ ë‚´ì´ë©´ í˜„ì¬ ì²­í¬ì— ì¶”ê°€
        should_continue = (
            similarity >= similarity_threshold and 
            current_tokens + sent_tokens <= CHUNK_SIZE_TOK
        )
        
        if should_continue:
            current_chunk.append(sent)
            current_tokens += sent_tokens
        else:
            # í˜„ì¬ ì²­í¬ ì™„ì„±í•˜ê³  ìƒˆ ì²­í¬ ì‹œì‘
            if current_chunk:
                chunks.append(" ".join(current_chunk)[:4000])
            current_chunk = [sent]
            current_tokens = sent_tokens
    
    # ë§ˆì§€ë§‰ ì²­í¬ ì²˜ë¦¬
    if current_chunk:
        chunks.append(" ".join(current_chunk)[:4000])
    
    return chunks


def agentic_chunk_text(text: str, model_name: str = "models/gemini-2.5-flash") -> List[str]:
    """
    LLM ê¸°ë°˜ ì—ì´ì „í‹± ì²­í‚¹
    
    Gemini ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ì˜ ìì—°ìŠ¤ëŸ¬ìš´ ë…¼ë¦¬ì  ë‹¨ìœ„ë¥¼ ì‹ë³„í•˜ê³ 
    ì£¼ì œ ì „í™˜ì , ì„¹ì…˜ ê²½ê³„, ë…¼ì¦ êµ¬ì¡° ë“±ì„ ê³ ë ¤í•œ ì§€ëŠ¥ì  ë¶„í•  ìˆ˜í–‰.
    
    Args:
        text: ë¶„í• í•  ì›ë³¸ í…ìŠ¤íŠ¸
        model_name: ì‚¬ìš©í•  Gemini ëª¨ë¸ëª…
    
    Returns:
        ë…¼ë¦¬ì  ì¼ê´€ì„±ì„ ê°–ì¶˜ ì²­í¬ í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
    """
    if genai is None:
        print("âš ï¸ Agentic chunking requires google-generativeai. Falling back to simple chunking.")
        return chunk_text(text)
    
    key = os.getenv("GOOGLE_API_KEY")
    if not key:
        print("âš ï¸ GOOGLE_API_KEY not set. Falling back to simple chunking.")
        return chunk_text(text)
    
    # í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ì§§ìœ¼ë©´ ë‹¨ìˆœ ì²­í‚¹ ì‚¬ìš©
    if len(text) < 2000:
        return chunk_text(text)
    
    try:
        genai.configure(api_key=key)
        model = genai.GenerativeModel(model_name)
        
        # LLMì—ê²Œ í…ìŠ¤íŠ¸ ë¶„í•  ì§€ì‹œí•˜ëŠ” í”„ë¡¬í”„íŠ¸
        prompt = f"""
ë‹¤ìŒ í•™ìˆ  ë…¼ë¬¸ í…ìŠ¤íŠ¸ë¥¼ ì˜ë¯¸ì ìœ¼ë¡œ ì¼ê´€ëœ ì²­í¬ë“¤ë¡œ ë¶„í• í•´ì£¼ì„¸ìš”.

ë¶„í•  ê¸°ì¤€:
1. ì£¼ì œ ì „í™˜ì  ì‹ë³„
2. ë…¼ë¦¬ì  ë‹¨ìœ„ (ì„œë¡ , ë°©ë²•ë¡ , ê²°ê³¼, ê²°ë¡  ë“±) ê³ ë ¤  
3. ê° ì²­í¬ëŠ” 500-1000 í† í° ì •ë„ê°€ ì ì ˆ
4. ë¬¸ì¥ ì¤‘ê°„ì—ì„œ ìë¥´ì§€ ë§ê³  ì™„ì „í•œ ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„í• 

ì¶œë ¥ í˜•ì‹:
ê° ì²­í¬ë¥¼ "---CHUNK---" êµ¬ë¶„ìë¡œ ë¶„ë¦¬í•˜ì—¬ ì¶œë ¥

í…ìŠ¤íŠ¸:
{text[:8000]}
"""
        
        response = model.generate_content(prompt)
        result_text = response.text
        
        # LLM ì‘ë‹µì—ì„œ ì²­í¬ ì¶”ì¶œ
        if "---CHUNK---" in result_text:
            raw_chunks = result_text.split("---CHUNK---")
            chunks = []
            
            for chunk in raw_chunks:
                cleaned = chunk.strip()
                if len(cleaned) > 100:  # ë„ˆë¬´ ì§§ì€ ì²­í¬ ì œì™¸
                    # í† í° ìˆ˜ í™•ì¸í•˜ì—¬ ë„ˆë¬´ í¬ë©´ ì¬ë¶„í• 
                    if estimate_token_count(cleaned) > CHUNK_SIZE_TOK * 1.5:
                        chunks.extend(chunk_text(cleaned))
                    else:
                        chunks.append(cleaned[:4000])
            
            if chunks:  # LLMì´ ì„±ê³µì ìœ¼ë¡œ ë¶„í• í–ˆìœ¼ë©´ ë°˜í™˜
                return chunks
    
    except Exception as e:
        print(f"âš ï¸ Agentic chunking failed: {e}. Falling back to simple chunking.")
    
    # LLM ë¶„í•  ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ ì²­í‚¹ìœ¼ë¡œ í´ë°±
    return chunk_text(text)


def get_chunking_function(method: str):
    """
    ì²­í‚¹ ë°©ë²•ì— ë”°ë¥¸ í•¨ìˆ˜ ë°˜í™˜
    
    Args:
        method: "simple", "semantic", "agentic" ì¤‘ í•˜ë‚˜
    
    Returns:
        ì„ íƒëœ ì²­í‚¹ í•¨ìˆ˜
    """
    if method == "semantic":
        return semantic_chunk_text
    elif method == "agentic":
        return agentic_chunk_text
    else:
        return chunk_text  # ê¸°ë³¸ê°’: simple chunking


def build_index(chunking_method: str = "semantic", similarity_threshold: float = 0.5):
    """
    FAISS ì¸ë±ìŠ¤ êµ¬ì¶• ë©”ì¸ í•¨ìˆ˜
    
    ì²˜ë¦¬ ê³¼ì •:
    1. NLTK ë¦¬ì†ŒìŠ¤ ë‹¤ìš´ë¡œë“œ (punkt, punkt_tab)
    2. ë°ì´í„°ì…‹ ë¡œë“œ ë° í…ìŠ¤íŠ¸ ì²­í‚¹
    3. Sentence-Transformerë¡œ ì„ë² ë”© ìƒì„±
    4. FAISS ì¸ë±ìŠ¤ êµ¬ì¶• (ì½”ì‚¬ì¸ ìœ ì‚¬ë„ìš© ë‚´ì  ì¸ë±ìŠ¤)
    5. ì¸ë±ìŠ¤ì™€ ë©”íƒ€ë°ì´í„° ì €ì¥
    """
    # NLTK ë¦¬ì†ŒìŠ¤ ë‹¤ìš´ë¡œë“œ (ë¬¸ì¥ ë¶„í• ìš©)
    nltk.download('punkt', quiet=True)
    # NLTK 4.x í˜¸í™˜ì„±: punkt_tab ë¦¬ì†ŒìŠ¤ë„ í•„ìš”í•  ìˆ˜ ìˆìŒ
    try:
        nltk.data.find('tokenizers/punkt_tab')
    except LookupError:
        nltk.download('punkt_tab', quiet=True)
    
    print("ğŸ“– Loading datasetâ€¦")
    
    # ë°ì´í„°ì…‹ íŒŒì¼ ì¡´ì¬ í™•ì¸
    if not DATA_FILE.exists():
        print(f"âŒ {DATA_FILE} not found.")
        sys.exit(1)

    # Sentence-Transformer ëª¨ë¸ ë¡œë“œ
    model = SentenceTransformer(EMB_MODEL_NAME)
    dim = model.get_sentence_embedding_dimension()  # ì„ë² ë”© ì°¨ì› ìˆ˜ (384)
    
    # ì²­í‚¹ ë°©ë²• ì„ íƒ ë° ì„¤ì •
    print(f"ğŸ”§ Using {chunking_method} chunking method")
    chunking_func = get_chunking_function(chunking_method)
    
    # FAISS ì¸ë±ìŠ¤ ì´ˆê¸°í™” (ë‚´ì /ì½”ì‚¬ì¸ ìœ ì‚¬ë„ìš©)
    # IndexFlatIP: ì •ê·œí™”ëœ ë²¡í„°ì— ëŒ€í•´ ë‚´ì  = ì½”ì‚¬ì¸ ìœ ì‚¬ë„
    index = faiss.IndexFlatIP(dim)
    metadata: List[str] = []  # ê° ì„ë² ë”©ì— ëŒ€ì‘í•˜ëŠ” í…ìŠ¤íŠ¸ ì²­í¬ ì €ì¥

    def batch(iterable, n=64):
        """
        ì´í„°ëŸ¬ë¸”ì„ ì§€ì •ëœ í¬ê¸°ì˜ ë°°ì¹˜ë¡œ ë¶„í• í•˜ëŠ” ì œë„ˆë ˆì´í„°
        ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ ë°°ì¹˜ ì²˜ë¦¬ë¥¼ ìœ„í•¨
        """
        it = iter(iterable)
        while True:
            chunk = list(itertools.islice(it, n))
            if not chunk:
                return
            yield chunk

    # JSONL ë°ì´í„°ì…‹ì„ ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ ì²˜ë¦¬ (ë©”ëª¨ë¦¬ ì ˆì•½)
    with gzip.open(DATA_FILE, "rt", encoding="utf-8") as f:
        for line in tqdm(f, desc="parsing"):
            obj = json.loads(line)
            # ë‹¤ì–‘í•œ í•„ë“œëª… ì§€ì› (full_text, text ë“±)
            txt = obj.get("full_text") or obj.get("text") or ""
            if not txt:
                continue
            
            # ì„ íƒëœ ë°©ë²•ìœ¼ë¡œ í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë¶„í• í•˜ì—¬ ë©”íƒ€ë°ì´í„°ì— ì¶”ê°€
            if chunking_method == "semantic":
                # ì˜ë¯¸ì  ì²­í‚¹: ì„ë² ë”© ëª¨ë¸ê³¼ ìœ ì‚¬ë„ ì„ê³„ê°’ ì „ë‹¬
                chunks = chunking_func(txt, model, similarity_threshold=similarity_threshold)
            elif chunking_method == "agentic":
                # ì—ì´ì „í‹± ì²­í‚¹: LLM ëª¨ë¸ëª… ì „ë‹¬
                chunks = chunking_func(txt, model_name="models/gemini-2.5-flash")
            else:
                # ë‹¨ìˆœ ì²­í‚¹: í…ìŠ¤íŠ¸ë§Œ ì „ë‹¬
                chunks = chunking_func(txt)
            
            for chunk in chunks:
                metadata.append(chunk)
    
    print(f"ğŸ§© Total chunks: {len(metadata):,}")

    # ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì„ë² ë”© ìƒì„± ë° ì¸ë±ìŠ¤ì— ì¶”ê°€
    # 64ê°œì”© ì²˜ë¦¬í•˜ì—¬ GPU ë©”ëª¨ë¦¬ íš¨ìœ¨ì„± í™•ë³´
    for texts in tqdm(batch(metadata, 64), total=len(metadata)//64 + 1, desc="embedding"):
        # ì„ë² ë”© ìƒì„± í›„ ì •ê·œí™” (ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°ì„ ìœ„í•´)
        emb = model.encode(texts, convert_to_numpy=True, normalize_embeddings=True)
        # FAISS ì¸ë±ìŠ¤ì— ì¶”ê°€
        index.add(emb)

    # ì¸ë±ìŠ¤ì™€ ë©”íƒ€ë°ì´í„°ë¥¼ ë””ìŠ¤í¬ì— ì €ì¥
    faiss.write_index(index, str(INDEX_FILE))
    with open(META_FILE, "wb") as f:
        pickle.dump(metadata, f)
    
    print(f"âœ… Index saved to {INDEX_FILE}, metadata to {META_FILE}")


def ask(query: str, top_k: int = 5):
    """
    RAG ì§ˆì˜ì‘ë‹µ í•¨ìˆ˜
    
    ì²˜ë¦¬ ê³¼ì •:
    1. ì§ˆì˜ í…ìŠ¤íŠ¸ë¥¼ ì„ë² ë”©ìœ¼ë¡œ ë³€í™˜
    2. FAISSì—ì„œ ìœ ì‚¬í•œ ì²­í¬ top_kê°œ ê²€ìƒ‰
    3. ê²€ìƒ‰ëœ ì²­í¬ë“¤ì„ ì»¨í…ìŠ¤íŠ¸ë¡œ Geminiì— ì „ë‹¬
    4. LLMì´ ìƒì„±í•œ ë‹µë³€ ì¶œë ¥
    
    Args:
        query: ì‚¬ìš©ì ì§ˆë¬¸
        top_k: ê²€ìƒ‰í•  ìƒìœ„ ì²­í¬ ìˆ˜
    """
    # ì¸ë±ìŠ¤ íŒŒì¼ ì¡´ì¬ í™•ì¸
    if not INDEX_FILE.exists():
        print("âŒ Index file not found. Run with 'build' first.")
        return
    
    # ì €ì¥ëœ ì¸ë±ìŠ¤ì™€ ë©”íƒ€ë°ì´í„° ë¡œë“œ
    index = faiss.read_index(str(INDEX_FILE))
    with open(META_FILE, "rb") as f:
        meta = pickle.load(f)
    
    # ì§ˆì˜ ì„ë² ë”© ìƒì„± (ë™ì¼í•œ ëª¨ë¸ ì‚¬ìš©)
    model = SentenceTransformer(EMB_MODEL_NAME)
    q_emb = model.encode([query], normalize_embeddings=True)
    
    # FAISSì—ì„œ ìœ ì‚¬í•œ ì²­í¬ ê²€ìƒ‰
    # D: ìœ ì‚¬ë„ ì ìˆ˜, I: ì¸ë±ìŠ¤ ë²ˆí˜¸
    D, I = index.search(q_emb, top_k)
    
    # ê²€ìƒ‰ëœ ì²­í¬ë“¤ì„ ì»¨í…ìŠ¤íŠ¸ë¡œ ê²°í•©
    ctx = "\n\n".join(meta[i] for i in I[0])
    
    # RAG í”„ë¡¬í”„íŠ¸ êµ¬ì„±
    prompt = f"Answer the following question using the provided context. If the context does not contain the answer, say 'I don't know.'\n\nContext:\n{ctx}\n\nQuestion: {query}"

    # Gemini API í‚¤ í™•ì¸
    key = os.getenv("GOOGLE_API_KEY")
    if not key:
        print("â„¹ï¸  GOOGLE_API_KEY not set. Returning only retrieved context.")
        print("\n----- Context -----\n", ctx[:2000])  # ì»¨í…ìŠ¤íŠ¸ë§Œ ì¶œë ¥
        return
    
    # google-generativeai íŒ¨í‚¤ì§€ í™•ì¸
    if genai is None:
        print("pip install google-generativeai required.")
        return
    
    # Gemini API í˜¸ì¶œ
    genai.configure(api_key=key)
    gem = genai.GenerativeModel("models/gemini-2.5-pro")  # ìµœì‹  Gemini Pro ëª¨ë¸
    resp = gem.generate_content(prompt)
    
    # ë‹µë³€ ì¶œë ¥
    print("\n----- Answer -----\n")
    print(resp.text)


if __name__ == "__main__":
    import argparse

    # ì»¤ë§¨ë“œë¼ì¸ ì¸í„°í˜ì´ìŠ¤ ì„¤ì •
    parser = argparse.ArgumentParser()
    sub = parser.add_subparsers(dest="cmd", required=True)

    # 'build' ì„œë¸Œì»¤ë§¨ë“œ: ì¸ë±ìŠ¤ êµ¬ì¶•
    build_p = sub.add_parser("build")
    build_p.add_argument("--chunking", choices=["simple", "semantic", "agentic"], 
                        default="semantic", help="ì²­í‚¹ ë°©ë²• ì„ íƒ (ê¸°ë³¸ê°’: semantic)")
    build_p.add_argument("--similarity-threshold", type=float, default=0.5,
                        help="ì˜ë¯¸ì  ì²­í‚¹ ìœ ì‚¬ë„ ì„ê³„ê°’ (0.0-1.0, ê¸°ë³¸ê°’: 0.5)")
    
    # 'ask' ì„œë¸Œì»¤ë§¨ë“œ: ì§ˆì˜ì‘ë‹µ
    ask_p = sub.add_parser("ask")
    ask_p.add_argument("question", nargs="+")  # ì§ˆë¬¸ (ì—¬ëŸ¬ ë‹¨ì–´ ê°€ëŠ¥)
    ask_p.add_argument("--top_k", type=int, default=5, help="ê²€ìƒ‰í•  ìƒìœ„ ì²­í¬ ìˆ˜")
    ask_p.add_argument("--show_ctx", action="store_true", help="Gemini í˜¸ì¶œ ì—†ì´ ì»¨í…ìŠ¤íŠ¸ë§Œ ì¶œë ¥")

    args = parser.parse_args()

    if args.cmd == "build":
        # ë¹Œë“œ ì‹œ ì²­í‚¹ ë°©ë²• ì„¤ì •
        build_index(chunking_method=args.chunking, similarity_threshold=args.similarity_threshold)
    else:
        # ì§ˆë¬¸ ë¬¸ìì—´ ì¬êµ¬ì„±
        q = " ".join(args.question)
        if args.show_ctx:
            # ì»¨í…ìŠ¤íŠ¸ë§Œ ë³´ê¸° ëª¨ë“œ: Gemini í‚¤ë¥¼ ì œê±°í•˜ì—¬ LLM í˜¸ì¶œ ê±´ë„ˆë›°ê¸°
            os.environ.pop("GOOGLE_API_KEY", None)
        ask(q, top_k=args.top_k) 