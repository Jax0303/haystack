#!/usr/bin/env python3
# create_realistic_dataset.py
import gzip
import json
import random
import os
from datetime import datetime, timedelta

INPUT_FILE = "./combined_papers_2024_25.jsonl.gz"
OUTPUT_FILE = "./rag_dataset_2024_25.jsonl.gz"
TARGET_SIZE_MB = 400

# ë…¼ë¬¸ ì„¹ì…˜ë³„ í…œí”Œë¦¿ (ë” í˜„ì‹¤ì ì¸ ê¸¸ì´)
SECTION_TEMPLATES = {
    "introduction": [
        "Recent advances in {field} have demonstrated remarkable progress in addressing complex challenges. This paper presents novel approaches to {topic}, building upon established methodologies while introducing innovative techniques that address current limitations in the field.",
        "The rapid evolution of {field} has created new opportunities for research in {topic}. Our work contributes to this growing body of knowledge by proposing methodologies that overcome existing bottlenecks and provide enhanced performance metrics.",
        "In the context of {field}, the problem of {topic} has gained significant attention due to its practical implications and theoretical importance. This research addresses key gaps in current understanding through comprehensive experimental validation."
    ],
    "related_work": [
        "Previous studies in {field} have explored various aspects of {topic}. Smith et al. (2023) proposed a foundational approach that established key principles, while Johnson and Lee (2024) extended this work with novel algorithmic improvements. However, these approaches face limitations in scalability and generalization.",
        "The literature on {topic} within {field} reveals several distinct research directions. Classical methods focused on traditional optimization techniques, while recent work has embraced machine learning approaches. Our method synthesizes insights from both paradigms.",
        "Comprehensive surveys of {field} highlight the evolution of {topic} research over the past decade. Early work emphasized theoretical foundations, whereas contemporary research prioritizes practical applications and real-world deployment considerations."
    ],
    "methodology": [
        "Our approach to {topic} employs a multi-stage methodology combining theoretical analysis with empirical validation. The core algorithm leverages advanced {field} techniques to achieve optimal performance while maintaining computational efficiency.",
        "We propose a novel framework for {topic} that integrates multiple {field} components. The methodology consists of three primary phases: data preprocessing, model training, and evaluation. Each phase incorporates domain-specific optimizations.",
        "The experimental design follows rigorous scientific principles to ensure reproducibility and validity. Our {topic} implementation utilizes state-of-the-art {field} libraries and frameworks, with careful attention to hyperparameter optimization and cross-validation procedures."
    ],
    "results": [
        "Experimental evaluation demonstrates significant improvements over baseline methods in {field}. Our approach to {topic} achieves 15-25% better performance metrics across standard benchmarks, with particularly strong results in challenging test scenarios.",
        "Comprehensive testing reveals the effectiveness of our {topic} methodology in {field} applications. Statistical analysis confirms the significance of observed improvements, with p-values consistently below 0.05 across all evaluation metrics.",
        "Comparative analysis against state-of-the-art methods shows superior performance of our {topic} approach. The results indicate robust generalization capabilities and consistent behavior across diverse {field} datasets and experimental conditions."
    ],
    "conclusion": [
        "This work makes several important contributions to {field} research in {topic}. We have demonstrated the viability of our approach through extensive experimentation and analysis. Future research directions include exploring applications to related domains and investigating theoretical properties.",
        "In conclusion, our research advances the state-of-the-art in {topic} within the {field} domain. The proposed methodology addresses key limitations of existing approaches while opening new avenues for investigation. Practical implications extend to real-world applications.",
        "The results presented in this paper establish new benchmarks for {topic} research in {field}. Our contributions include both theoretical insights and practical improvements that will benefit the broader research community. Ongoing work focuses on scalability and deployment considerations."
    ]
}

def generate_realistic_content(paper):
    """ë…¼ë¬¸ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ í˜„ì‹¤ì ì¸ ê¸´ ë‚´ìš© ìƒì„±"""
    title = paper.get("title", "")
    abstract = paper.get("abstract", "")
    categories = paper.get("categories", [])
    
    # ë¶„ì•¼ ì¶”ì •
    field = "machine learning"
    if categories:
        if any("cv" in str(cat).lower() or "vision" in str(cat).lower() for cat in categories):
            field = "computer vision"
        elif any("nlp" in str(cat).lower() or "cl" in str(cat).lower() for cat in categories):
            field = "natural language processing"
        elif any("ai" in str(cat).lower() for cat in categories):
            field = "artificial intelligence"
        elif any("robotics" in str(cat).lower() or "ro" in str(cat).lower() for cat in categories):
            field = "robotics"
    
    # ì£¼ì œ ì¶”ì¶œ (ì œëª©ì—ì„œ)
    topic = title.lower().replace(":", "").replace(",", "")
    
    # ê° ì„¹ì…˜ ìƒì„±
    sections = []
    
    # Abstract (ê¸°ì¡´ ê²ƒ ì‚¬ìš©í•˜ë˜ í™•ì¥)
    if abstract:
        expanded_abstract = f"{abstract} This research addresses fundamental challenges in {field} and proposes innovative solutions with broad applicability."
        sections.append(f"Abstract: {expanded_abstract}")
    
    # Introduction (ì—¬ëŸ¬ ë¬¸ë‹¨)
    intro_texts = []
    for template in SECTION_TEMPLATES["introduction"]:
        intro_texts.append(template.format(field=field, topic=topic))
    sections.append(f"1. Introduction\n\n" + "\n\n".join(intro_texts))
    
    # Related Work (ì—¬ëŸ¬ ë¬¸ë‹¨)
    related_texts = []
    for template in SECTION_TEMPLATES["related_work"]:
        related_texts.append(template.format(field=field, topic=topic))
    sections.append(f"2. Related Work\n\n" + "\n\n".join(related_texts))
    
    # Methodology (ì—¬ëŸ¬ ë¬¸ë‹¨)
    method_texts = []
    for template in SECTION_TEMPLATES["methodology"]:
        method_texts.append(template.format(field=field, topic=topic))
    sections.append(f"3. Methodology\n\n" + "\n\n".join(method_texts))
    
    # Results (ì—¬ëŸ¬ ë¬¸ë‹¨)
    result_texts = []
    for template in SECTION_TEMPLATES["results"]:
        result_texts.append(template.format(field=field, topic=topic))
    sections.append(f"4. Results\n\n" + "\n\n".join(result_texts))
    
    # Conclusion (ì—¬ëŸ¬ ë¬¸ë‹¨)
    conclusion_texts = []
    for template in SECTION_TEMPLATES["conclusion"]:
        conclusion_texts.append(template.format(field=field, topic=topic))
    sections.append(f"5. Conclusion\n\n" + "\n\n".join(conclusion_texts))
    
    # ì¶”ê°€ ì„¹ì…˜ë“¤
    sections.append(f"6. Experimental Setup\n\nDetailed experimental protocols for {topic} in {field} were designed to ensure reproducibility. The evaluation framework incorporates multiple metrics and validation procedures to assess performance comprehensively.")
    
    sections.append(f"7. Discussion\n\nThe implications of our findings for {field} research are significant. The proposed approach to {topic} demonstrates clear advantages over existing methods while highlighting areas for future investigation.")
    
    # ì „ì²´ ë…¼ë¬¸ í…ìŠ¤íŠ¸
    full_text = "\n\n".join(sections)
    
    # ë…¼ë¬¸ ì •ë³´ ì—…ë°ì´íŠ¸
    enhanced_paper = paper.copy()
    enhanced_paper["full_text"] = full_text
    enhanced_paper["word_count"] = len(full_text.split())
    enhanced_paper["estimated_pages"] = len(full_text.split()) // 250  # ëŒ€ëµ 250ë‹¨ì–´/í˜ì´ì§€
    
    return enhanced_paper

def load_and_expand_papers():
    """ê¸°ì¡´ ë…¼ë¬¸ë“¤ì„ ë¡œë“œí•˜ê³  ë‚´ìš© í™•ì¥"""
    print(f"ğŸ“– {INPUT_FILE}ì—ì„œ ë…¼ë¬¸ ë¡œë”©...")
    
    papers = []
    with gzip.open(INPUT_FILE, "rt", encoding="utf-8") as f:
        for line in f:
            paper = json.loads(line.strip())
            # ì œëª©ê³¼ ì´ˆë¡ì´ ìˆëŠ” ë…¼ë¬¸ë§Œ ì²˜ë¦¬
            if paper.get("title") and paper.get("abstract"):
                enhanced = generate_realistic_content(paper)
                papers.append(enhanced)
    
    print(f"âœ… {len(papers)}í¸ ë…¼ë¬¸ ë¡œë“œ ë° í™•ì¥ ì™„ë£Œ")
    return papers

def create_target_size_dataset(papers, target_mb):
    """ëª©í‘œ í¬ê¸°ì— ë§ëŠ” ë°ì´í„°ì…‹ ìƒì„±"""
    print(f"ğŸ¯ ëª©í‘œ í¬ê¸° {target_mb} MB ë°ì´í„°ì…‹ ìƒì„±...")
    
    # ë¨¼ì € ìƒ˜í”Œë¡œ í¬ê¸° ì¶”ì •
    sample_paper = papers[0]
    sample_size = len(json.dumps(sample_paper, ensure_ascii=False).encode('utf-8'))
    estimated_papers_needed = int((target_mb * 1024 * 1024) / sample_size)
    
    print(f"ğŸ“Š ë…¼ë¬¸ë‹¹ í‰ê·  í¬ê¸°: {sample_size/1024:.1f} KB")
    print(f"ğŸ“Š ì˜ˆìƒ í•„ìš” ë…¼ë¬¸ ìˆ˜: {estimated_papers_needed}í¸")
    
    # ê¸°ì¡´ ë…¼ë¬¸ë“¤ë¡œ ëª©í‘œ í¬ê¸°ê¹Œì§€ ë°˜ë³µ
    final_papers = []
    current_size = 0
    target_size = target_mb * 1024 * 1024
    
    while current_size < target_size:
        for paper in papers:
            if current_size >= target_size:
                break
                
            # ì•½ê°„ì˜ ë³€í˜•ì„ ê°€í•´ì„œ ì¤‘ë³µ ë°©ì§€
            modified_paper = paper.copy()
            modified_paper["paper_id"] = f"{paper.get('paper_id', 'unknown')}_{len(final_papers)}"
            
            final_papers.append(modified_paper)
            current_size += len(json.dumps(modified_paper, ensure_ascii=False).encode('utf-8'))
            
            if len(final_papers) % 100 == 0:
                current_mb = current_size / (1024 * 1024)
                print(f"  ğŸ“Š {len(final_papers)}í¸ ìƒì„±, {current_mb:.1f} MB")
    
    return final_papers

def main():
    print("ğŸš€ í˜„ì‹¤ì ì¸ RAG ì‹¤í—˜ìš© ë°ì´í„°ì…‹ ìƒì„± ì‹œì‘...")
    
    # ë…¼ë¬¸ ë¡œë“œ ë° í™•ì¥
    papers = load_and_expand_papers()
    
    if not papers:
        print("âŒ ì‚¬ìš© ê°€ëŠ¥í•œ ë…¼ë¬¸ì´ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # ëª©í‘œ í¬ê¸° ë°ì´í„°ì…‹ ìƒì„±
    final_papers = create_target_size_dataset(papers, TARGET_SIZE_MB)
    
    # ì €ì¥
    print(f"ğŸ’¾ {OUTPUT_FILE}ì— ì €ì¥ ì¤‘...")
    with gzip.open(OUTPUT_FILE, "wt", encoding="utf-8") as f:
        for paper in final_papers:
            f.write(json.dumps(paper, ensure_ascii=False) + "\n")
    
    # ìµœì¢… í†µê³„
    final_size_mb = os.path.getsize(OUTPUT_FILE) / (1024 * 1024)
    print(f"\nğŸ‰ ë°ì´í„°ì…‹ ìƒì„± ì™„ë£Œ!")
    print(f"ğŸ“Š ì´ ë…¼ë¬¸ ìˆ˜: {len(final_papers)}í¸")
    print(f"ğŸ“ íŒŒì¼ í¬ê¸°: {final_size_mb:.1f} MB")
    print(f"ğŸ’¡ ë…¼ë¬¸ë‹¹ í‰ê·  í¬ê¸°: {final_size_mb*1024/len(final_papers):.1f} KB")
    
    # ìƒ˜í”Œ í™•ì¸
    print(f"\nğŸ“„ ì²« ë²ˆì§¸ ë…¼ë¬¸ ìƒ˜í”Œ (ì²˜ìŒ 500ì):")
    sample = final_papers[0]
    print(f"ì œëª©: {sample.get('title', '')}")
    print(f"ì „ë¬¸: {sample.get('full_text', '')[:500]}...")

if __name__ == "__main__":
    main() 