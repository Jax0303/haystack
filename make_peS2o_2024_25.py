# make_peS2o_2024_25.py
from datasets import load_dataset
import gzip, json, os, tqdm

CACHE_DIR = "./hf_cache"                 # Hugging Face ìºì‹œ
OUT_PATH  = "./peS2o_2024_25.jsonl.gz"  # ìµœì¢… ì €ì¥ íŒŒì¼

print("ğŸ“š Loading peS2o dataset in streaming mode...")

# ------------ 1) ìŠ¤íŠ¸ë¦¬ë° ë¡œë“œ ------------
ds = load_dataset(
    "allenai/peS2o", "v2",
    split="train",
    streaming=True,
    cache_dir=CACHE_DIR
)

# ------------ 2) ì—°ë„ í•„í„° ì •ì˜ ------------
def is_2024_25(ex):
    date = ex.get("created", "")         # YYYY-MM-DD
    return date.startswith("2024") or date.startswith("2025")

filtered = ds.filter(is_2024_25)

print(f"ğŸ” Filtering for 2024-2025 papers...")
print(f"ğŸ’¾ Saving to {OUT_PATH}")

# ------------ 3) gzipìœ¼ë¡œ ì €ì¥ ------------
count = 0
with gzip.open(OUT_PATH, "wt", encoding="utf-8") as f_out:
    for ex in tqdm.tqdm(filtered, desc="Processing papers"):
        # í•„ìš”í•œ í•„ë“œë§Œ ì €ì¥ (ìš©ëŸ‰ ì ˆì•½)
        paper = {
            "paper_id": ex.get("paper_id", ""),
            "title": ex.get("title", ""),
            "abstract": ex.get("abstract", ""),
            "body_text": ex.get("body_text", ""),
            "created": ex.get("created", ""),
            "venue": ex.get("venue", ""),
            "authors": ex.get("authors", [])
        }
        f_out.write(json.dumps(paper, ensure_ascii=False) + "\n")
        count += 1
        
        # ì§„í–‰ìƒí™© ì¤‘ê°„ ì²´í¬
        if count % 1000 == 0:
            size_mb = os.path.getsize(OUT_PATH) / (1024**2)
            print(f"  ğŸ“Š {count} papers processed, {size_mb:.1f} MB so far")

size_mb = os.path.getsize(OUT_PATH) / (1024**2)
print(f"\nâœ… Complete! Saved {count} papers ({size_mb:.1f} MB) to {OUT_PATH}")
print(f"ğŸ“ˆ Average paper size: {size_mb*1024/count:.1f} KB per paper") 